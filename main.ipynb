{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcd71690",
   "metadata": {},
   "source": [
    "# Cuaderno de soporte para IL-CLDM\n",
    "Este cuaderno concentra los comandos auxiliares que usamos para entrenar y diagnosticar el modelo de difusión latente condicionado (IL-CLDM).\n",
    "- **Entrenamiento**: controla la ejecución del script principal y de cada etapa (AAE, codificador de latentes y difusor).\n",
    "- **Diagnóstico**: agrupa rutinas de inspección para verificar datos, métricas y distribuciones latentes.\n",
    "> Recomendación: revisa/ajusta `config.py` antes de ejecutar cualquier celda y avanza en orden según la sección que necesites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1543b916",
   "metadata": {},
   "source": [
    "## 1. Ejecución y entrenamiento principal\n",
    "Las celdas siguientes lanzan el entrenamiento desde el notebook. Ejecuta solo las que necesites:\n",
    "1. `!python main.py` corre el flujo completo desde la terminal (entrena AAE, codifica latentes y ajusta el LDM).\n",
    "2. `main.train_AAE()` entrena únicamente el autoencoder adversario.\n",
    "3. `main.encoding()` genera/actualiza los latentes SPECT tras entrenar el AAE.\n",
    "4. `main.train_LDM()` entrena el modelo de difusión condicionado usando los latentes generados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701c707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura variables de entorno compartidas para las ejecuciones posteriores.\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b5f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecuta la tubería completa (AAE + codificación + LDM) desde la línea de comandos.\n",
    "!python main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f01f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permite invocar funciones específicas del pipeline directamente desde Python.\n",
    "import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983e7544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrena solo la primera etapa (AAE) sin necesidad de correr todo `main.py`.\n",
    "main.train_AAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b634448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera/actualiza los latentes SPECT una vez ajustado el AAE.\n",
    "main.encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72270669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empieza el entrenamiento del modelo de difusión condicionado usando los latentes almacenados.\n",
    "main.train_LDM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88778d55",
   "metadata": {},
   "source": [
    "## 2. Evaluación cuantitativa del AAE en SPECT\n",
    "Usa estas celdas para comparar reconstrucciones con los datos reales mediante PSNR/SSIM y ejecutar `aae_eval.py` cuando necesites verificar la convergencia de la primera etapa antes de continuar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbbb0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nibabel as nib, numpy as np, torch\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import config\n",
    "from model import AAE\n",
    "from utils import load_checkpoint\n",
    "from dataset import center_crop\n",
    "\n",
    "device = torch.device(config.device)\n",
    "out_dir = os.path.join(\"result\", str(config.exp), \"aae_recon_test\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "latent_dir = config.latent_Abeta\n",
    "sample_id = os.listdir(latent_dir)[0]  # o elige uno\n",
    "latent_path = os.path.join(latent_dir, sample_id)\n",
    "\n",
    "lat = nib.load(latent_path).get_fdata().astype(np.float32)\n",
    "x_lat = torch.tensor(lat[None,None,...], device=device)\n",
    "aae = AAE().to(device)\n",
    "opt = torch.optim.Adam(aae.parameters(), lr=config.learning_rate)\n",
    "load_checkpoint(config.CHECKPOINT_AAE, aae, opt, config.learning_rate)\n",
    "aae.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    recon = aae.decoder(x_lat)\n",
    "recon = torch.clamp(recon, 0, 1).cpu().numpy().squeeze().astype(np.float32)\n",
    "\n",
    "orig_id = os.path.splitext(sample_id)[0]\n",
    "orig_path = os.path.join(config.whole_Abeta, orig_id + \".nii\")\n",
    "if not os.path.exists(orig_path):\n",
    "    orig_path = orig_path + \"gz\"  # si es .nii.gz\n",
    "orig_img = nib.load(orig_path)\n",
    "orig = center_crop(orig_img.get_fdata().astype(np.float32), config.crop_size)\n",
    "vmin, vmax = orig.min(), orig.max()\n",
    "orig = np.zeros_like(orig) if vmax - vmin < 1e-8 else (orig - vmin)/(vmax - vmin)\n",
    "rng = max(orig.max() - orig.min(), 1e-8)\n",
    "print(\"PSNR\", psnr(orig, recon, data_range=rng), \"SSIM\", ssim(orig, recon, data_range=rng))\n",
    "\n",
    "nib.save(nib.Nifti1Image(recon, orig_img.affine),\n",
    "         os.path.join(out_dir, f\"{orig_id}_decoder_only.nii.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f328e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, nibabel as nib, csv, torch\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import config\n",
    "from dataset import resolve_nifti, center_crop  # usa center_crop\n",
    "from model import AAE\n",
    "\n",
    "device = torch.device(config.device)\n",
    "model = AAE().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "ckpt = torch.load(config.CHECKPOINT_AAE, map_location=device)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "ids = [i.strip() for i in open(config.test) if i.strip()]\n",
    "psnr_sum = ssim_sum = 0\n",
    "\n",
    "with open(\"result/exp_1/aae_recon_test.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f); writer.writerow([\"ID\",\"PSNR\",\"SSIM\"])\n",
    "    for bid in ids:\n",
    "        path = resolve_nifti(config.whole_Abeta, bid)\n",
    "        img = nib.load(path)\n",
    "        data = img.get_fdata().astype(np.float32)\n",
    "        data = center_crop(data, config.crop_size)\n",
    "        vmin, vmax = data.min(), data.max()\n",
    "        data = np.zeros_like(data) if vmax - vmin < 1e-8 else (data - vmin)/(vmax - vmin)\n",
    "\n",
    "        x = torch.tensor(data[None,None,...], device=device)\n",
    "        with torch.no_grad():\n",
    "            recon = model(x)\n",
    "        recon = torch.clamp(recon, 0, 1).cpu().numpy().squeeze().astype(np.float32)\n",
    "\n",
    "        # Asegúrate de que coinciden las formas\n",
    "        if recon.shape != data.shape:\n",
    "            print(f\"{bid} shapes mismatch: data {data.shape}, recon {recon.shape}\")\n",
    "            continue\n",
    "\n",
    "        rng = max(data.max() - data.min(), 1e-8)\n",
    "        ps = psnr(data, recon, data_range=rng)\n",
    "        ss = ssim(data, recon, data_range=rng)\n",
    "        writer.writerow([bid, ps, ss])\n",
    "        psnr_sum += ps; ssim_sum += ss\n",
    "\n",
    "print(\"Promedio PSNR\", psnr_sum/len(ids), \"SSIM\", ssim_sum/len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076b77cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "aae_results = pd.read_csv(\"/workspace/projects/T1-SPECT-translation/IL-CLDM/result/exp_1/aae_recon_test/aae_metrics.csv\")\n",
    "aae_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99821b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aae_results[\"PSNR\"].mean(), aae_results[\"PSNR\"].std(), aae_results[\"SSIM\"].mean(), aae_results[\"SSIM\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58312aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python aae_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e5c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nibabel as nib, numpy as np, torch\n",
    "gen_path = \"/workspace/projects/T1-SPECT-translation/IL-CLDM/data/latent_SPECT\"\n",
    "files = os.listdir(gen_path)\n",
    "\n",
    "for file in files:\n",
    "    #getting the shape of each file\n",
    "    latent = nib.load(os.path.join(gen_path, file)).get_fdata()\n",
    "    if latent.shape != (40, 48, 40):\n",
    "        print(f\"File: {file}, Shape: {latent.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148f37c3",
   "metadata": {},
   "source": [
    "## 3. Exploración de la representación latente de MRI\n",
    "Permite inspeccionar los archivos latentes generados por el encoder para confirmar dimensiones (`config.latent_shape`) y rangos antes de alimentar el difusor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7167084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nibabel as nib, torch\n",
    "from dataset import resolve_nifti, center_crop, z_score_norm\n",
    "from model import UNet\n",
    "import config\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(config.device)\n",
    "\n",
    "# Carga el UNet con el image_size correcto\n",
    "unet = UNet(in_channel=2, out_channel=1, image_size=config.latent_shape[0]).to(device)\n",
    "unet.eval()\n",
    "\n",
    "ids = [i.strip() for i in open(config.train) if i.strip()][:3]  # prueba con 3 IDs\n",
    "out_dir = \"result/exp_unet_sd/mri_latent_cond\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bid in ids:\n",
    "        mri_path = resolve_nifti(config.whole_MRI, bid)\n",
    "        img = nib.load(mri_path)\n",
    "        mri = img.get_fdata().astype(np.float32)\n",
    "        # mismo preprocesado que en TwoDataset\n",
    "        from dataset import resample_to_shape\n",
    "        mri = resample_to_shape(mri, config.target_shape)\n",
    "        mri = center_crop(mri, config.crop_size)\n",
    "        mri = z_score_norm(mri)\n",
    "\n",
    "        x = torch.tensor(mri[None, None, ...], device=device)\n",
    "        cond_latent = unet.cond(x).cpu().numpy().squeeze()  # esperado: 40x48x40 con config actual\n",
    "\n",
    "        print(bid, \"cond shape\", cond_latent.shape, \"min/max\", cond_latent.min(), cond_latent.max())\n",
    "        nib.save(nib.Nifti1Image(cond_latent, img.affine),\n",
    "                 os.path.join(out_dir, f\"{bid}_mri_cond.nii.gz\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c33c3",
   "metadata": {},
   "source": [
    "## 4. Verificación de datos de entrada\n",
    "Aquí puedes abrir volúmenes MRI/SPECT brutos, revisar sus minas/máximos y asegurarte de que la nomenclatura de archivos coincida con `data_info/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19917c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "base_path = \"/workspace/projects/T1-SPECT-translation/IL-CLDM/data/whole_SPECT\"\n",
    "\n",
    "files = os.listdir(base_path)\n",
    "nan_values = []\n",
    "\n",
    "for file in files:\n",
    "    if file.endswith(\".nii\"):\n",
    "        filepath = os.path.join(base_path, file)\n",
    "        img = nib.load(filepath)\n",
    "        data = img.get_fdata()\n",
    "        print(\"dimansions of the file:\", data.shape)\n",
    "        if np.isnan(data).any():\n",
    "            print(f\"El archivo {file} contiene NaNs.\")\n",
    "            #cuantos NaNs hay\n",
    "            num_nans = np.isnan(data).sum()\n",
    "            print(f\"Número de NaNs en el archivo {file}: {num_nans}\")\n",
    "            nan_values.append((file, num_nans))\n",
    "        else:\n",
    "            print(f\"El archivo {file} no contiene NaNs.\")\n",
    "\n",
    "#obtener el file con menos NaNs y el de mas NaNs\n",
    "if nan_values:\n",
    "    nan_values.sort(key=lambda x: x[1])\n",
    "    print(f\"Archivo con menos NaNs: {nan_values[0][0]} con {nan_values[0][1]} NaNs\")\n",
    "    print(f\"Archivo con más NaNs: {nan_values[-1][0]} con {nan_values[-1][1]} NaNs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837d64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"3863.nii\"\n",
    "filepath = os.path.join(base_path, file)\n",
    "img = nib.load(filepath)\n",
    "data = img.get_fdata()\n",
    "print(\"dimansions of the file:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b9632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def resolve(root, bid):\n",
    "    for ext in ('.nii', '.nii.gz'):\n",
    "        p = Path(root)/f\"{bid}{ext}\"\n",
    "        if p.exists(): return p\n",
    "    raise FileNotFoundError(bid)\n",
    "\n",
    "def stats(path):\n",
    "    data = nib.load(str(path)).get_fdata()\n",
    "    data = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return data.min(), data.max(), data.mean(), data.std()\n",
    "\n",
    "root_abeta = Path('data/whole_SPECT')\n",
    "root_mri = Path('data/whole_MRI')\n",
    "train_ids = Path('data_info/train.txt').read_text().splitlines()[:-10]  # cambia [:3] por más\n",
    "print('IDs', train_ids)\n",
    "for bid in train_ids:\n",
    "    pA = resolve(root_abeta, bid)\n",
    "    pM = resolve(root_mri, bid)\n",
    "    amin, amax, amean, astd = stats(pA)\n",
    "    mmin, mmax, mmean, mstd = stats(pM)\n",
    "    print(f\"{bid} Abeta: min {amin:.4f} max {amax:.4f} mean {amean:.4f} std {astd:.4f}\")\n",
    "    print(f\"{bid} MRI  : min {mmin:.4f} max {mmax:.4f} mean {mmean:.4f} std {mstd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19fec72",
   "metadata": {},
   "source": [
    "## 5. Métricas locales PSNR/SSIM\n",
    "Fragmentos rápidos para validar manualmente las métricas sobre casos concretos o ruido sintético, útiles cuando se ajustan ventanas o normalizaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9741f34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from dataset import OneDataset\n",
    "import config\n",
    "\n",
    "ds = OneDataset(root_Abeta=config.whole_Abeta, task=config.validation, stage=\"validation\")\n",
    "for i in range(3):  # primeros 3\n",
    "    vol, name = ds[i]\n",
    "    vol = vol.astype(np.float32)\n",
    "    rng = max(vol.max() - vol.min(), 1e-8)\n",
    "    print(name, \"range\", vol.min(), vol.max(),\n",
    "          \"PSNR self\", psnr(vol, vol, data_range=rng),\n",
    "          \"SSIM self\", ssim(vol, vol, data_range=rng))\n",
    "    # Volumen con ruido para ver caída\n",
    "    noise = np.random.normal(0, 0.01, size=vol.shape).astype(np.float32)\n",
    "    vol_noisy = np.clip(vol + noise, 0, 1)\n",
    "    print(\"  noisy PSNR\", psnr(vol, vol_noisy, data_range=rng),\n",
    "          \"noisy SSIM\", ssim(vol, vol_noisy, data_range=rng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nibabel as nib, numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import config\n",
    "\n",
    "ids = open(config.validation).read().split()\n",
    "for bid in ids[:10]:  # revisa los primeros 10\n",
    "    path = None\n",
    "    for ext in (\".nii\", \".nii.gz\"):\n",
    "        p = os.path.join(config.whole_Abeta, bid + ext)\n",
    "        if os.path.exists(p):\n",
    "            path = p\n",
    "            break\n",
    "    if not path:\n",
    "        print(\"no file for\", bid)\n",
    "        continue\n",
    "    img = nib.load(path).get_fdata().astype(np.float32)\n",
    "    vol = img[10:170, 18:210, 10:170]  # mismo crop\n",
    "    vmin, vmax = vol.min(), vol.max()\n",
    "    vol = np.zeros_like(vol) if vmax - vmin < 1e-8 else (vol - vmin) / (vmax - vmin)\n",
    "    rng = max(vol.max() - vol.min(), 1e-8)\n",
    "    psnr_self = psnr(vol, vol, data_range=rng)\n",
    "    ssim_self = ssim(vol, vol, data_range=rng)\n",
    "    print(bid, \"range\", vol.min(), vol.max(), \"psnr_self\", psnr_self, \"ssim_self\", ssim_self)\n",
    "    if np.isfinite(ssim_self) and ssim_self > 0.9:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4879a7",
   "metadata": {},
   "source": [
    "## 6. Curvas de entrenamiento y validación\n",
    "Carga los CSV generados durante el entrenamiento (`loss_curve.csv`, `validation.csv`, etc.) para visualizar la evolución de pérdidas y métricas por época."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ad4a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09917117",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_path = \"/workspace/projects/T1-SPECT-translation/IL-CLDM/result/exp_unet_sd\"\n",
    "file1 = \"loss_curve.csv\"\n",
    "file2 = \"validation.csv\"\n",
    "\n",
    "loss_df = pd.read_csv(os.path.join(gen_path, file1))\n",
    "similarity_df = pd.read_csv(os.path.join(gen_path, file2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3323ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda3515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67d5904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = loss_df\n",
    "\n",
    "# --- Convertir columnas a numérico (forzando errores a NaN) ---\n",
    "for col in df.columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"ignore\")\n",
    "\n",
    "# Si epoch, PSNR o SSIM están como strings, forzamos:\n",
    "df[\"Epoch\"] = pd.to_numeric(df[\"Epoch\"], errors=\"coerce\")\n",
    "df[\"recon_loss\"] = pd.to_numeric(df[\"recon_loss\"], errors=\"coerce\")\n",
    "df[\"disc_loss_epoch\"] = pd.to_numeric(df[\"disc_loss_epoch\"], errors=\"coerce\")\n",
    "\n",
    "# Eliminar filas que quedaron con NaN en métricas o epochs\n",
    "df = df.dropna(subset=[\"Epoch\", \"recon_loss\", \"disc_loss_epoch\"])\n",
    "\n",
    "# Redondear PSNR y SSIM a 2 decimales\n",
    "df[\"recon_loss\"] = df[\"recon_loss\"].round(2)\n",
    "df[\"disc_loss_epoch\"] = df[\"disc_loss_epoch\"].round(2)\n",
    "\n",
    "# Crear figura\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Graficar PSNR\n",
    "plt.plot(df[\"Epoch\"], df[\"recon_loss\"], marker='o', label=\"recon_loss\", linewidth=2)\n",
    "\n",
    "# Graficar SSIM\n",
    "plt.plot(df[\"Epoch\"], df[\"disc_loss_epoch\"], marker='s', label=\"disc_loss_epoch\", linewidth=2)\n",
    "\n",
    "# Títulos y ejes\n",
    "plt.title(\"recon_loss y disc_loss_epoch vs Épocas\")\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.ylabel(\"Valor Métrica\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03cbb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = similarity_df\n",
    "\n",
    "# --- Convertir columnas a numérico (forzando errores a NaN) ---\n",
    "for col in df.columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"ignore\")\n",
    "\n",
    "# Si epoch, PSNR o SSIM están como strings, forzamos:\n",
    "df[\"Epoch\"] = pd.to_numeric(df[\"Epoch\"], errors=\"coerce\")\n",
    "df[\"PSNR\"] = pd.to_numeric(df[\"PSNR\"], errors=\"coerce\")\n",
    "df[\"SSIM\"] = pd.to_numeric(df[\"SSIM\"]*100, errors=\"coerce\")\n",
    "\n",
    "# Eliminar filas que quedaron con NaN en métricas o epochs\n",
    "df = df.dropna(subset=[\"Epoch\", \"PSNR\", \"SSIM\"])\n",
    "\n",
    "# Redondear PSNR y SSIM a 2 decimales\n",
    "df[\"PSNR\"] = df[\"PSNR\"].round(2)\n",
    "df[\"SSIM\"] = df[\"SSIM\"].round(2)\n",
    "\n",
    "# Crear figura\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Graficar PSNR\n",
    "plt.plot(df[\"Epoch\"], df[\"PSNR\"], marker='o', label=\"PSNR\", linewidth=2)\n",
    "\n",
    "# Graficar SSIM\n",
    "plt.plot(df[\"Epoch\"], df[\"SSIM\"], marker='s', label=\"SSIM\", linewidth=2)\n",
    "\n",
    "# Títulos y ejes\n",
    "plt.title(\"PSNR y SSIM vs Épocas\")\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.ylabel(\"Valor Métrica\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acca89c8",
   "metadata": {},
   "source": [
    "## 7. Conversión de .nii.gz a .nii\n",
    "Utiliza estas celdas cuando necesites trabajar con herramientas que solo aceptan `.nii`, manteniendo una copia limpia en el directorio de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5826d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import gzip\n",
    "import shutil\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc8cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"/workspace/projects/T1-SPECT-translation/IL-CLDM/data/whole_SPECT\"\n",
    "output_folder = \"/workspace/projects/T1-SPECT-translation/IL-CLDM/data/whole_SPECT2\"\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for f in Path(input_folder).glob(\"*.nii.gz\"):\n",
    "    img = nib.load(str(f))\n",
    "    out_path = Path(output_folder) / f.with_suffix('').name  # quita .gz → .nii\n",
    "    nib.save(img, str(out_path))\n",
    "    print(\"Guardado:\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d1311",
   "metadata": {},
   "source": [
    "## 8. Inferencia con el modelo entrenado\n",
    "Lanza `infer.py` para generar volúmenes sintéticos usando los checkpoints actuales. Ideal para validar resultados cualitativos tras el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c08e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python infer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c95d07",
   "metadata": {},
   "source": [
    "## 9. Carga manual del UNet de difusión\n",
    "Proporciona utilidades para inspeccionar la arquitectura, cargar pesos y depurar tensores del modelo de difusión acondicionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a727ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import UNet\n",
    "import config\n",
    "\n",
    "device = torch.device(config.device)\n",
    "unet = UNet(in_channel=2, out_channel=1, image_size=config.latent_shape[0]).to(device)\n",
    "print(unet)  # lista todos los módulos, incluidas las capas con atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaf5b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "from model import UNet\n",
    "import config\n",
    "\n",
    "device = torch.device(config.device)\n",
    "latent_shape = config.latent_shape       # p.ej. (40,48,40)\n",
    "crop_shape   = config.crop_size          # p.ej. (160,192,160)\n",
    "\n",
    "unet = UNet(in_channel=2, out_channel=1, image_size=latent_shape[0]).to(device)\n",
    "\n",
    "x_dummy = torch.randn(1, 1, *latent_shape, device=device)   # latente SPECT\n",
    "mri_dummy = torch.randn(1, 1, *crop_shape, device=device)    # MRI recortada\n",
    "t_dummy = torch.tensor([10], device=device, dtype=torch.long)\n",
    "label_dummy = torch.tensor([0], device=device, dtype=torch.long)  # o None si no usas label\n",
    "\n",
    "summary(unet,\n",
    "        input_data=(x_dummy, mri_dummy, t_dummy, label_dummy),\n",
    "        depth=3)  # ajusta depth si quieres menos/más detalle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e71dc02",
   "metadata": {},
   "source": [
    "## 10. Estadísticas de los latentes SPECT y MRI\n",
    "Reúne scripts para calcular histogramas, medias y desviaciones estándar tanto de los latentes reales como de los sintetizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca4031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nibabel as nib, numpy as np, config\n",
    "import glob\n",
    "\n",
    "files = glob.glob(os.path.join(config.latent_Abeta, \"*.nii*\"))  # muestrea 100\n",
    "vals = []\n",
    "for f in files:\n",
    "    x = nib.load(f).get_fdata().astype(np.float32)\n",
    "    vals.append(x.reshape(-1))\n",
    "vals = np.concatenate(vals)\n",
    "print(\"Latente SPECT shape esperado\", config.latent_shape)\n",
    "print(\"min\", vals.min(), \"max\", vals.max(), \"mean\", vals.mean(), \"std\", vals.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6477eb2f",
   "metadata": {},
   "source": [
    "### 10.1 Estadísticas del acondicionamiento MRI\n",
    "Calcula estadísticas básicas de los tensores de acondicionamiento derivados del MRI para asegurarse de que comparten rango con los latentes SPECT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3693b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, nibabel as nib, numpy as np, config\n",
    "from dataset import resolve_nifti, resample_to_shape, center_crop, z_score_norm\n",
    "from model import UNet\n",
    "\n",
    "device = torch.device(config.device)\n",
    "unet = UNet(in_channel=2, out_channel=1, image_size=config.latent_shape[0]).to(device)\n",
    "unet.eval()\n",
    "\n",
    "ids = [i.strip() for i in open(config.train) if i.strip()] \n",
    "vals = []\n",
    "with torch.no_grad():\n",
    "    for bid in ids:\n",
    "        mri_path = resolve_nifti(config.whole_MRI, bid)\n",
    "        mri = nib.load(mri_path).get_fdata().astype(np.float32)\n",
    "        mri = resample_to_shape(mri, config.target_shape)\n",
    "        mri = center_crop(mri, config.crop_size)\n",
    "        mri = z_score_norm(mri)\n",
    "        x = torch.tensor(mri[None,None,...], device=device)\n",
    "        cond = unet.cond(x).cpu().numpy().squeeze()  # debería ser 40×48×40\n",
    "        vals.append(cond.reshape(-1))\n",
    "vals = np.concatenate(vals)\n",
    "print(\"Cond MRI shape\", cond.shape)\n",
    "print(\"min\", vals.min(), \"max\", vals.max(), \"mean\", vals.mean(), \"std\", vals.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7139b0",
   "metadata": {},
   "source": [
    "### 10.2 Estadísticas de latentes sintetizados\n",
    "Evalúa los latentes generados por el difusor (después de muestrear `ema_Unet`) para confirmar que mantienen la distribución objetivo antes de decodificar a espacio imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf71884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, nibabel as nib, numpy as np, torch\n",
    "import config\n",
    "from model import AAE, UNet\n",
    "from main import Diffusion\n",
    "from dataset import resolve_nifti, resample_to_shape, center_crop, z_score_norm\n",
    "\n",
    "device = torch.device(config.device)\n",
    "\n",
    "# Carga modelos\n",
    "aae = AAE().to(device)\n",
    "opt = torch.optim.Adam(aae.parameters(), lr=config.learning_rate)\n",
    "aae_ckpt = torch.load(config.CHECKPOINT_AAE, map_location=device)\n",
    "aae.load_state_dict(aae_ckpt[\"state_dict\"])\n",
    "aae.eval()\n",
    "\n",
    "unet = UNet(in_channel=2, out_channel=1, image_size=config.latent_shape[0]).to(device)\n",
    "opt_u = torch.optim.AdamW(unet.parameters(), lr=config.learning_rate)\n",
    "print(\"Cargando UNet desde\", config.CHECKPOINT_Unet)\n",
    "state = torch.load(config.CHECKPOINT_Unet, map_location=device)\n",
    "sd = state.get(\"unet_state_dict\") or state.get(\"state_dict\")\n",
    "# quitar prefijo module. si existe\n",
    "sd = {k.replace(\"module.\", \"\", 1): v for k, v in sd.items()}\n",
    "unet.load_state_dict(sd, strict=False)\n",
    "unet.eval()\n",
    "diffusion = Diffusion()\n",
    "\n",
    "# Elige algunas MRI\n",
    "ids = [i.strip() for i in open(config.train) if i.strip()][:15]\n",
    "latents = []\n",
    "with torch.no_grad():\n",
    "    for bid in ids:\n",
    "        mri_path = resolve_nifti(config.whole_MRI, bid)\n",
    "        mri = nib.load(mri_path).get_fdata().astype(np.float32)\n",
    "        mri = resample_to_shape(mri, config.target_shape)\n",
    "        mri = center_crop(mri, config.crop_size)\n",
    "        mri = z_score_norm(mri)\n",
    "        x = torch.tensor(mri[None,None,...], device=device)\n",
    "        # sample latent from diffusion\n",
    "        samp_lat = diffusion.sample(unet, x)\n",
    "        latents.append(samp_lat.cpu().numpy().reshape(-1))\n",
    "\n",
    "latents = np.concatenate(latents)\n",
    "print(\"Latente difusión -> decoder:\")\n",
    "print(\"shape esperado\", config.latent_shape, \"min\", latents.min(), \"max\", latents.max(), \"mean\", latents.mean(), \"std\", latents.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78825ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = np.concatenate(latents)\n",
    "print(\"Latente difusión -> decoder:\")\n",
    "print(\"shape esperado\", config.latent_shape, \"min\", latents.min(), \"max\", latents.max(), \"mean\", latents.mean(), \"std\", latents.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616cb920",
   "metadata": {},
   "source": [
    "## 11. Impacto del condicionamiento por etiqueta\n",
    "Reserva estas celdas para experimentos donde se compare el entrenamiento con/ sin `label` como condición adicional en el UNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0051650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añade aquí los experimentos cuantitativos sobre el condicionamiento de etiquetas cuando estén listos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
